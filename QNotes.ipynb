{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = float(max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam:\n",
    "    # 就是将上面的代码进行了封装\n",
    "\n",
    "    def __init__(self,num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max is None or self.max <tensor.max():\n",
    "            self.max = tensor.max()\n",
    "        self.max = 0 if self.max < 0 else self.max   #保证self.max大于等于0\n",
    "\n",
    "        if self.min is None or self.min >tensor.min():\n",
    "            self.min = tensor.min()\n",
    "        self.min = 0 if self.min > 0 else self.min   #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(min_val=tensor.min(),max_val=tensor.max(),num_bits=8)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def quantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([-50.6078, -33.4922, -31.6157, -44.8647, -37.3588])\n",
      "q_y= tensor([-33.5490, -33.5490,  11.9412, -58.0000, -31.8431, -27.8627, -39.2353,\n",
      "        -70.5098])\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "y = torch.tensor([20,20,100,-23,23,30,10,-45])\n",
    "q_parm.update(x)\n",
    "q_parm.update(y)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_y=\",q_parm.quantize_tensor(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "    \n",
    "    def __init__(self,q_input=True,q_output=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        q_ipnut:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        q_output: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        self.num_bits = num_bits\n",
    "        if q_input:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if q_output:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(q_input=qi,q_output=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M = self.qw.scale*self.qi.scale / self.qo.scale\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  ？？？为什么减去zero_point\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 进行量化和反量化\n",
    "        # 实际上是伪量化节点，模拟量化前后的误差\n",
    "        # 这样的float推理和量化后的int推理具有相同的精度\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data)\n",
    "        self.conv_module.weight.data = self.qw.dequantize_tensor(self.conv_module.weight.data)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x + self.qo.zero_point\n",
    "\n",
    "        return x\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dfq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bb67ba0ae6effbc54f1e26c9156de107350f6c04280f0ca82fd14e38b65dbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
