{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = (max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))   #量化误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam(nn.Module):\n",
    "    # 就是将上面的代码进行了封装\n",
    "    # 继承Module是为了让量化（参数）成为网络的一部分\n",
    "    def __init__(self,num_bits=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bits = num_bits\n",
    "        scale = torch.tensor([], requires_grad=False)\n",
    "        zero_point = torch.tensor([], requires_grad=False)\n",
    "        min = torch.tensor([], requires_grad=False)\n",
    "        max = torch.tensor([], requires_grad=False)\n",
    "        \n",
    "        # 使用register_buffer保存量化参数有以下优点：\n",
    "        # 不产生梯度，不会被注册到parameters中，但也会保存到state_dict中\n",
    "        # 这样就能把模型权重和量化参数都独立地保存到模型里。\n",
    "        self.register_buffer('scale', scale)  \n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        self.register_buffer('min', min)\n",
    "        self.register_buffer('max', max)\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max.nelement() == 0 or self.max <tensor.max():\n",
    "            self.max.data = tensor.max().data   #使用data赋值，避免self.max对象本身\n",
    "        self.max.clamp_(min=0)  #保证self.max大于等于0\n",
    "\n",
    "        if self.min.nelement() == 0  or self.min >tensor.min():\n",
    "            self.min.data = tensor.min().data\n",
    "        self.min.clamp_(max=0)  #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(self.min, self.max, self.num_bits)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def dequantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n",
    "\n",
    "    # 从状态字典加载量化参数\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
    "        key_names = ['scale', 'zero_point', 'min', 'max']\n",
    "        for key in key_names:\n",
    "            value = getattr(self, key)\n",
    "            value.data = state_dict[prefix + key].data\n",
    "            state_dict.pop(prefix + key)\n",
    "\n",
    "    def __str__(self):\n",
    "        info = 'scale:%.10f '  % self.scale\n",
    "        info += 'zero_point:%d '  % self.zero_point\n",
    "        info += 'min:%.6f '  % self.min\n",
    "        info += 'max:%.6f'  % self.max\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([-11.2643,  -7.3218,  -6.8896,  -9.9414,  -8.2125])\n",
      "q_parm: scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n",
      "q_parm state dict: OrderedDict([('scale', tensor(0.1310)), ('zero_point', tensor(76.)), ('min', tensor(-10.)), ('max', tensor(23.4000))])\n",
      "q_parm_new scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "q_parm.update(x)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_parm:\",q_parm)  #打印量化参数\n",
    "print(\"q_parm state dict:\",q_parm.state_dict()) #打印状态字典\n",
    "torch.save(q_parm.state_dict(),\"./q_parm.pt\")# 保存状态字典\n",
    "\n",
    "q_parm_new = QParam(num_bits=8)  #创建新的量化参数对象\n",
    "q_parm_new.load_state_dict(torch.load(\"./q_parm.pt\")) #从保存的状态字典中加载量化参数\n",
    "print(\"q_parm_new\",q_parm_new)  #打印量化参数，看是否一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import math\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "\n",
    "    def __init__(self,qi=True,qo=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        qi:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        qo: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n",
    "\n",
    "class FakeQuantize(Function):\n",
    "    # 伪量化节点，进行量化和反量化\n",
    "    # 模拟量化前后的误差，这样的float推理和量化后的int推理具有相同的精度\n",
    "\n",
    "    # 反向传播求梯度使用STE，这部分在PTQ不进行反向传播，暂时可以忽略。  \n",
    "    # Function类似于没有参数的Module，继承需要重写前向传播forward和反向传播backward\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        \"\"\"\n",
    "        def forward(ctx,input,*args)\n",
    "            ctx: 不需要手动传入，能够执行一些操作方便求梯度，例如：\n",
    "            ctx.save_for_backward(tensor)  在前向传播时保存一些张量\n",
    "            tensor = ctx.saved_tensors    在反向传播时（backward函数内）也可访问到\n",
    "\n",
    "            input:函数的输入\n",
    "            *args:其他可选参数\n",
    "        \"\"\"\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output:后一层传来的梯度\n",
    "\n",
    "        在QAT涉及到反向传播\n",
    "        对于本层（伪量化节点）,不计算梯度，直接把后一层的梯度往前传\n",
    "\n",
    "        backward的返回值需要和forward对应，代表对应输入的梯度。\n",
    "        由于q_parm为量化参数，不需要计算梯度，因此返回None\n",
    "        因此直接 return grad_output，None\n",
    "        \"\"\"\n",
    "        return grad_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(q_input=qi,q_output=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "        self.num_bits = num_bits\n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M = self.qw.scale*self.qi.scale / self.qo.scale\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  为什么减去zero_point? 其实就是对应公式里 q_w-Z_w\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "        # 推理过程中顺便统计计算输入x，输出a，权重w的量化参数\n",
    "        # 后量化通过数据校准实现这些。\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qi)  #对q_x进行伪量化\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 对卷积权重q_w进行伪量化，然后和x计算卷积操作\n",
    "        # 注意卷积模块的权重始终是存储原始的weight，只有在前向传播时进行伪量化。\n",
    "        # 反向传播时也是更新的量化前的weight\n",
    "        x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, \n",
    "                     stride=self.conv_module.stride,\n",
    "                     padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                     groups=self.conv_module.groups)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qo)  #对输出a做伪量化\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point  #q_x - Z_x\n",
    "        x = self.conv_module(x)    #(q_w-Z_w)*(q_x-Z_x)\n",
    "        x = self.M * x             #M*sum((q_w-Z_w)*(q_x-Z_x))\n",
    "        x.round_()                 #提前做一步round，加快速度。后面只剩下一个整数加法。\n",
    "        x = x + self.qo.zero_point   #M*sum((q_w-Z_w)*(q_x-Z_x))+Z_a\n",
    "        x.round_()\n",
    "        return x\n",
    "\n",
    "# 相同原理 实现其他网络模块\n",
    "class QLinear(QModule):\n",
    "    #量化线性层\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight.data = self.qw.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.qi.scale * self.qw.scale,\n",
    "                                                   zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight.data)\n",
    "\n",
    "        x = F.linear(x, FakeQuantize.apply(self.fc_module.weight, self.qw), self.fc_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()\n",
    "        return x\n",
    "\n",
    "\n",
    "class QReLU(QModule):\n",
    "    # 构建量化版的Relu函数\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        \n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n",
    "\n",
    "class QMaxPooling2d(QModule):\n",
    "    # 构建量化版的MaxPooling2d函数\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv-BN-Relu融合层的实现\n",
    "一个标准的Conv+BN+relu可以合并到一起，加快推理速度。\n",
    "- **BN折叠到卷积层**  \n",
    "卷积层的输出如下所示:  $$y=\\sum_{i}^{N} w_{i} x_{i}+b$$\n",
    "BN层的输出如下所示:  \n",
    "$$y_{bn} =\\gamma \\frac{y-\\mu_{y}}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}+\\beta$$\n",
    "将卷积层的输出$y$带入到BN层中得到：  \n",
    "$$y_{b n}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}\\left(\\sum_{i}^{N} w_{i} x_{i}+b-\\mu_{y}\\right)+\\beta$$\n",
    "仔细观察，当训练结束后，BN层统计量$\\mu_{y}$ ,$\\sigma_{y}$以及参数$\\gamma$, $\\beta$都已经固定下来。我们可以令$\\gamma^{\\prime}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}$,那么能够得到：\n",
    "$$y_{b n}=\\sum_{i}^{N} \\gamma^{\\prime} w_{i} x_{i}+\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$$\n",
    "上式已经和卷积计算公式非常像了，为了看得更清楚，我们令$w_{i}^{\\prime} = \\gamma^{\\prime}w_{i}$,  $b^{\\prime}=\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$,得到最终的BN层输出\n",
    " $$y_{bn}=\\sum_{i}^{N} w_{i} x_{i}^{\\prime}+b^{\\prime}$$\n",
    "这就和卷积的操作一模一样，因此我们可以把BN层折叠合并到卷积层中。先进行数值变换，在进行矩阵运算。\n",
    "\n",
    "- **Relu折叠到卷积层**  \n",
    "1. 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。  \n",
    "2. 要想保证浮点型Relu和量化型Relu数值的一致性，需要对输入输出使用相同的量化参数,否则无法反量化回正确的实数域。  \n",
    "3. relu会对数值进行截断，实际上在量化过程中也存在截断，就是把scale和偏移zero point后的值截断到（qmin,qmax）。  \n",
    "4. 对于单独的一个Relu层，我们可以直接使用输入的qi在量化数值空间做截断，$relu(q_{x}) = max(q_{x},Z_{x})$。\n",
    "5. 对于合并到conv中的relu层，我们可以使用**输出的qo作为量化参数，对relu的输入x进行量化**，量化的截断过程可以直接实现relu的操作。这是因为在float域，relu的值域是[0,r_max]，对应的量化值域就是[q_min,q_max]。relu函数在float域中，对小于0的数置为0；在量化int域中，就是对小于q_min的数置为q_min。而使用qo对x进行量化，就能保证这一点。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    # 构建量化BN层（与Relu融合）\n",
    "    def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.qb = QParam(num_bits=32)\n",
    "\n",
    "    def fold_bn(self, mean, std):\n",
    "        # 将BN层折叠到Conv层\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std  #r' = r/ sqrt(sigma^2+e)  实际上除以的标准差，加上e是为了避免分母为零\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) #w' = r'*w\n",
    "            if self.conv_module.bias is not None:         # b' = r' * b - r' * mu_y + beta\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias    #返回新的w和b\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        # 训练过程中，更新BN层参数\n",
    "        if self.training:\n",
    "            y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, \n",
    "                            stride=self.conv_module.stride,\n",
    "                            padding=self.conv_module.padding,\n",
    "                            dilation=self.conv_module.dilation,\n",
    "                            groups=self.conv_module.groups)\n",
    "            y = y.permute(1, 0, 2, 3) # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -> C,NHW\n",
    "            # mean = y.mean(1)\n",
    "            # var = y.var(1)\n",
    "            mean = y.mean(1).detach()\n",
    "            var = y.var(1).detach()\n",
    "            self.bn_module.running_mean = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                (1 - self.bn_module.momentum) * var\n",
    "        else:\n",
    "            mean = Variable(self.bn_module.running_mean)\n",
    "            var = Variable(self.bn_module.running_var)\n",
    "\n",
    "        std = torch.sqrt(var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(mean, std)  #折叠后的w和b\n",
    "\n",
    "        self.qw.update(weight.data)\n",
    "\n",
    "        #这里的卷积运算实际上包含了BN层\n",
    "        x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, \n",
    "                stride=self.conv_module.stride,\n",
    "                padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                groups=self.conv_module.groups)\n",
    "\n",
    "        x = F.relu(x)    #后面接上relu函数\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        std = torch.sqrt(self.bn_module.running_var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(self.bn_module.running_mean, std)\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(weight.data)\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias.data = quantize_tensor(bias, scale=self.qi.scale * self.qw.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        \n",
    "        #推理时将Relu合并到量化过程\n",
    "        #这一行包含了relu函数的作用，将x截断到(qmin,qmax)非对称量化         \n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型的构建\n",
    "我们在上面已经得到了基本的网络模块（卷积、池化、线性层等）的量化版本，  \n",
    "现在我们尝试构建一个完整的带有BN层的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    # 定义简单的卷积神经网络，包含两层卷积、2层BN层，1层全连接层\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    # 普通的全精度前向传播，穿插了2次relu函数和最大池化操作\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 我们需要把卷积层、池化层、全连接层转为其量化版本\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    # 量化版本的前向传播，就是各量化网络模块forward的串行堆叠\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    # 对各种量化参数进行固定\n",
    "    # 除了第一层卷积需要qi，其他都是对上一层的qo复用为当前层的qi\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.qo)\n",
    "        self.qconv2.freeze(qi=self.qconv1.qo)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.qo)\n",
    "        self.qfc.freeze(qi=self.qconv2.qo)\n",
    "\n",
    "    # 实际推理中用到的函数，是对各量化模块的quantize_inference的堆叠，纯整数存储和运算\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.qi.quantize_tensor(x)\n",
    "        qx = self.qconv1.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.qo.dequantize_tensor(qx)   #最后一步要反量化，得到实数域的结果\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全精度模型训练\n",
    "这里直接使用仓库提供的 *train.py* 得到在mnist数据集上准确率为99%的FP32模型，权重保存在 *./ckpt/mnist_cnnbn.pt*  \n",
    "```shell\n",
    "python ./train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后训练量化（PTQ）\n",
    "这里会将一个全精度模型转为一个量化模型，并且测试其精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的全精度模型\n",
    "fp32_model = NetBN()\n",
    "fp32_model.load_state_dict(torch.load(\"./ckpt/mnist_cnnbn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是几个用于测试精度的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一定的图片数据进行校准，确定input和中间activation的量化参数\n",
    "# 实际上模型weight的校准并不需要数据，这里为了方便都放在quantize_forward中进行校准\n",
    "from torchvision import datasets,transforms\n",
    "test_batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def direct_quantize(model, test_loader):\n",
    "    # 直接量化，使用一些数据集来校准量化参数,使用200个batch的数据\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_forward(data)\n",
    "        if i % 200 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    # 全精度模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quantize_inference(model, test_loader):\n",
    "    # 量化模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_inference(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp32_model.eval()\n",
    "full_inference(fp32_model,test_loader)  #测试全精度的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n",
      "num_bits 1\n",
      "\n",
      "Test set: Quant Model Accuracy: 10%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 2\n",
      "\n",
      "Test set: Quant Model Accuracy: 30%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 3\n",
      "\n",
      "Test set: Quant Model Accuracy: 84%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 4\n",
      "\n",
      "Test set: Quant Model Accuracy: 98%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 5\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 6\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 7\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 8\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accuracy_list = []\n",
    "for num_bits in range(1,9):  #测试num_bits=[1-8]的准确率\n",
    "    model = copy.deepcopy(fp32_model)\n",
    "    model.quantize(num_bits=num_bits) # 将模型的各个网络模块转为其量化版本\n",
    "    model.eval()\n",
    "    direct_quantize(model,test_loader)  #进行量化校准\n",
    "    model.freeze()                       # 将量化参数固定\n",
    "    print(\"num_bits\",num_bits)\n",
    "    acc = quantize_inference(model, test_loader)  #测试量化准确率\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb/ElEQVR4nO3de3zU9Z3v8dcn9wuEWwKEgIJKKYhANCLWrdWD1hsV8NRetdSi9nS7XbU9tba7PnpOt3va9ex62r11S/FCvbWIJqC2Vku1rp4KApMAggiIXCaBBJA7IbfP/pHBBggQMpP8Zn7zfj4ePmZ+t5m3kHnzy3d+8x1zd0REJFwygg4gIiKJp3IXEQkhlbuISAip3EVEQkjlLiISQllBBwAoLi72kSNHBh1DRCSlLF++fKe7l3S2LSnKfeTIkSxbtizoGCIiKcXMNp9sm4ZlRERCSOUuIhJCKncRkRA6bbmb2cNmVm9mqzusG2hmL5vZ+tjtgA7bvmtmG8xsnZld01PBRUTk5Lpy5v4ocO1x6+4DFrv7aGBxbBkzGwd8Djg/dsy/m1lmwtKKiEiXnLbc3f01YPdxq6cD82L35wEzOqz/lbsfcfdNwAZgcmKiiohIV3V3zH2Iu9cBxG4Hx9aXAVs77Lcttu4EZnanmS0zs2UNDQ3djCEiIp1J9HXu1sm6TucUdvc5wByAiooKzTss0oG7497+4nH32C047evpsPzn+8fuS4f9T9hG7NgPt3Wy73HP0+bQdjSX/3n56Lo277hP7H5b+21X9m9f14XH9A6P2XZsrqPbjvmz7FBBp5vhvOMU6Mfve/yhHbf7cVtPeexxGz8ytC/TJgw7dbBu6G657zCzUnevM7NSoD62fhswosN+w4HaeAKKBKW5tY3Dza0cbmrlUFP77eHmlg73W4+738LhprYP9znU1EpjbJ8/32/fdqS5rb2QOLF0Jfysw2nwtAnDkqrcFwGzgB/Hbhd2WP+kmT0IDANGA0vjDSlyOgePtLB516EOZdxybDEfc7/lJOuPPba59czaNjPDKMjOJC8nk4KcTPKzM8mP3R9QkNN+P7YuLzuTDGt/kRsWu+XDV71x4jYzsKPbjz/uuHXty9bhcWLLXX0e2heOHpdhkHH0+A7LGR8uH7tPRod1H+6fcfL9DSMjoxuPGVtnHY/t8GfQmeO32XEDDh23H/8wdtzBdsy2U+/b205b7mb2FHAFUGxm24Dv017q881sNrAFuBnA3d82s/nAGqAF+Lq7t/ZQdhEAXnp7O9+rXMXOA02n3C/DoCAn68PC7Vi+/Quyyc/J+rB8OxbxsftnnXBsQXYWeTkZ5GRmBP6CFjnKkuFr9ioqKlxzy8iZ2tfYzA+eW8OC5dsYW1rEX15xLkX52eRnx8o4VsJH76t8JWzMbLm7V3S2LSkmDhM5U/9/406+/fRK6vYe5q+uPI+/njqanCx94FrkKJW7pJTG5lYeeHEdD7+xiVHFhSz42se48KwBpz9QJM2o3CVl1GzdwzfnV7Ox4SCzLj2b71z3UQpy9CMs0hm9MiTpNbe28a9/2MC/vrKBkj65PDZ7Mh8f3en3E4hIjMpdktr6Hfv55vwaVkX3clN5Gd+/8Xz65WcHHUsk6ancJSm1tTkPv7GJB363jsKcTH72xQu57oLSoGOJpAyVuySdrbsP8T+frmHJpt1cNXYwP7ppAiV9c4OOJZJSVO6SNNyd+cu28oPn1mBmPPDpCdx80XBdmy7SDSp3SQr1+xv57jOrWPxOPVPOGcj//fRERgwsCDqWSMpSuUvgfrOqjr+pXMWhplbunzaO2z42kowMna2LxEPlLoHZe6iZ7y9aTVV1LROG9+PBz0zkvMF9g44lEgoqdwnEa+82cO+ClTQcOMLdV43m61eeR3ampg8QSRSVu/SqQ00t/Og37/DYm5s5b3Af5nzpIiYM7x90LJHQUblLr1m+eTffml/D5t2HmP0Xo/j2NWPIy9b3p4v0BJW79LgjLa389Pfr+Y8/bqS0Xz5P3j6FS88dFHQskVBTuUuPWlu3j3t+Xc072/fzmYrh3D9tHH3zNH2ASE9TuUuPaG1z5rz2Hg++vI5++dnM/VIFV40bEnQskbShcpeEe3/nQb71dA3LN3/AtecP5e9njmdQH00fINKbVO6SMO7OE0u28PcvrCUr0/h/n53IjEllmj5AJAAqd0mI7XsbufeZlbz2bgMfH13MA5+eQGm//KBjiaQtlbvExd1ZVFPL/VWraWpt4++mn88tU87W2bpIwFTu0m27DzZxf9VqXlhVR/lZ/XnwM5MYVVwYdCwRQeUu3fSHd3bwnWdWsedQE9++ZgxfvfwcsjR9gEjSULnLGTlwpIUfPr+GX721lTFD+vLobRdz/rB+QccSkeOo3KXLlry3i289XUN0z2H+xyfO5Z6rR5ObpekDRJKRyl1Oq7G5lX96aR1zX9/EiAEFzP/qpVw8cmDQsUTkFFTuckqro3u559fVrK8/wBcvOYvvXT+Wwlz92IgkO71KpVPuzr/8YQP/vHg9AwtzeOS2i7lyzOCgY4lIF6ncpVO/e3s7D778LtMmlPLDGePpX5ATdCQROQMqd+nUsyuilPTN5SefnaRLHEVSkF61coI9h5p4ZV09N04cpmIXSVF65coJnl9ZR3OrM7O8LOgoItJNKnc5QVUkyujBfTh/WFHQUUSkm1Tucowtuw6xbPMHzCjXVL0iqUzlLseoqo4CMENDMiIpLa5yN7N7zOxtM1ttZk+ZWZ6ZDTSzl81sfex2QKLCSs9yd6oiUS4ZNZCy/pqLXSSVdbvczawM+Gugwt3HA5nA54D7gMXuPhpYHFuWFLBy217e23lQb6SKhEC8wzJZQL6ZZQEFQC0wHZgX2z4PmBHnc0gvqYxEycnK4LoLSoOOIiJx6na5u3sU+EdgC1AH7HX3l4Ah7l4X26cO6PQz62Z2p5ktM7NlDQ0N3Y0hCdLc2sZzNbVcNXYw/fKzg44jInGKZ1hmAO1n6aOAYUChmd3S1ePdfY67V7h7RUlJSXdjSIK8vn4nuw42MWOShmREwiCeYZmrgE3u3uDuzcCzwMeAHWZWChC7rY8/pvS0ykiU/gXZXKHJwURCIZ5y3wJMMbMCa78geiqwFlgEzIrtMwtYGF9E6WkHjrTw0prt3HBBKTlZujpWJAy6PXGYuy8xswXACqAFiABzgD7AfDObTfs/ADcnIqj0nBdXb6exuY2bLtSQjEhYxDUrpLt/H/j+cauP0H4WLymiKhJlxMB8LjxLH0kQCQv9Dp7mduxr5I2NO5k5SdMNiISJyj3NLayO4q7pBkTCRuWe5iojtUwc0Z9zSvoEHUVEEkjlnsbe2b6PtXX7mDlpWNBRRCTBVO5prCpSS2aGMW2iyl0kbFTuaaqtzVlYHeUTHymhuE9u0HFEJMFU7mnqzU27qNvbqDdSRUJK5Z6mqiJRCnMyuXrskKCjiEgPULmnocbmVn67ajvXji8lPycz6Dgi0gNU7mno92t3sP9Ii6YbEAkxlXsaqopEGVKUy5RzBgUdRUR6iMo9zew+2MSr6xqYPqmMzAxNNyASVir3NPP8ylpa2lxfyiEScir3NFMZifLRoX0ZN6wo6Cgi0oNU7mnk/Z0HiWzZo2vbRdKAyj2NVFVHMYPpmktGJPRU7mnC3amMRLn0nEGU9ssPOo6I9DCVe5qIbN3D5l2HNCQjkiZU7mmiKhIlNyuDa8cPDTqKiPQClXsaaG5t47maWq4aN4SivOyg44hIL1C5p4E/rmvgg0PN3KQhGZG0oXJPA5XVUQYUZHP5R0qCjiIivUTlHnL7Gpv5/ZodfGriMLIz9dctki70ag+5F1dt50hLm66SEUkzKveQq4xEGTmogPIR/YOOIiK9SOUeYrV7DvPmpl3MKC/DTDNAiqQTlXuILaqpxR3NACmShlTuIeXuVK6IcuFZ/RlZXBh0HBHpZSr3kFpbt591O/YzU2+kiqQllXtIVVVHycowbpigGSBF0pHKPYRa25yF1VGuGFPCwMKcoOOISABU7iH0p4272LHvCDPLhwcdRUQConIPocpIlL65WUwdOzjoKCISEJV7yBxuauXF1XVcd8FQ8rIzg44jIgFRuYfMS2u2c7CpVdMNiKS5uMrdzPqb2QIze8fM1prZpWY20MxeNrP1sdsBiQorp1cViVLaL48powYFHUVEAhTvmftPgRfd/aPARGAtcB+w2N1HA4tjy9ILdh44wmvrdzJ9UhkZGZpuQCSddbvczawIuBx4CMDdm9x9DzAdmBfbbR4wI76I0lXP19TS2ub64JKIxHXmfg7QADxiZhEzm2tmhcAQd68DiN3qko1eUhmJMq60iDFD+wYdRUQCFk+5ZwEXAj9z93LgIGcwBGNmd5rZMjNb1tDQEEcMAdjYcICabXt11i4iQHzlvg3Y5u5LYssLaC/7HWZWChC7re/sYHef4+4V7l5RUqKvf4vXwkgUM7hxkqYbEJE4yt3dtwNbzWxMbNVUYA2wCJgVWzcLWBhXQjktd6eyOspl5xYzpCgv6DgikgSy4jz+G8ATZpYDvAfcRvs/GPPNbDawBbg5zueQ01i++QO27j7M3VM/EnQUEUkScZW7u1cDFZ1smhrP48qZqYxEycvO4JrxQ4OOIiJJQp9QTXFNLW08v7KOT44bSp/ceH8RE5GwULmnuFfW1bP3cLOukhGRY6jcU1xVJMqgwhw+Pro46CgikkRU7ils7+FmFq+t51MTh5GVqb9KEfkzNUIK++2qOppa2zQkIyInULmnsGcjUc4pLmTC8H5BRxGRJKNyT1HbPjjE0k27mVlehplmgBSRY6ncU9TC6loApk/SkIyInEjlnoLcncpIlIqzB3DWoIKg44hIElK5p6C3a/exof4AMy/UWbuIdE7lnoIqI1GyM40bLigNOoqIJCmVe4ppaW1jUU0tV44ZTP+CnKDjiEiSUrmnmDc27qJh/xFd2y4ip6RyTzFVkShFeVlc+VF9e6GInJzKPYUcPNLCi6u3c8OEUvKyM4OOIyJJTOWeQl5es4PDza3M0LXtInIaKvcU8mwkSln/fC4eOTDoKCKS5FTuKaJ+fyOvr29gRvkwMjI03YCInJrKPUU8V1NHm6MhGRHpEpV7iqiKRBlfVsToIX2DjiIiKUDlngI21O9nVXQvM8uHBx1FRFKEyj0FVEaiZBh8aqKmGxCRrlG5J7m2NqcqUstfjC5hcN+8oOOISIpQuSe5t97fTXTPYWaWDws6ioikEJV7kquqjlKQk8k15w8NOoqIpBCVexJrbG7l+ZV1XHP+UApysoKOIyIpROWexF5dV8/+xhZmaAZIETlDKvck9uyKKMV9crns3EFBRxGRFKNyT1J7DjXxyrp6pk8aRlam/ppE5MyoNZLUC6vqaG51fSmHiHSLyj1JVUWinDe4D+cPKwo6ioikIJV7Etq6+xBvvf8BM8vLMNMMkCJy5lTuSagqEgVg+iR9cElEukflnmTcncrqKJNHDWT4gIKg44hIilK5J5mV2/byXsNBvZEqInFRuSeZykiUnMwMrr9AM0CKSPfFXe5mlmlmETN7PrY80MxeNrP1sdsB8cdMD82tbTxXU8vUsYPpl58ddBwRSWGJOHO/C1jbYfk+YLG7jwYWx5alC17fsJNdB5s03YCIxC2ucjez4cANwNwOq6cD82L35wEz4nmOdFK5Ikq//GyuGFMSdBQRSXHxnrn/BLgXaOuwboi71wHEbgd3dqCZ3Wlmy8xsWUNDQ5wxUt+BIy28tGY70yaUkpuVGXQcEUlx3S53M5sG1Lv78u4c7+5z3L3C3StKSnSm+rvV22lsbtNVMiKSEPFMEn4ZcKOZXQ/kAUVm9jiww8xK3b3OzEqB+kQEDbuq6igjBuZz0dl6/1lE4tftM3d3/667D3f3kcDngD+4+y3AImBWbLdZwMK4U4bcjn2NvLFhJzMnaboBEUmMnrjO/cfA1Wa2Hrg6tiynsKi6ljaH6RqSEZEESch3t7n7q8Crsfu7gKmJeNx0URmJMnF4P84t6RN0FBEJCX1CNWDrtu9nTd0+XdsuIgmlcg9YZSRKZobxqYmaAVJEEkflHqC2NmdhdZTLRxdT3Cc36DgiEiIq9wAt2bSbur2NGpIRkYRTuQeoMrKNwpxMPjluaNBRRCRkVO4BaWxu5bertnPt+FLyczTdgIgklso9IIvX1rP/SIumGxCRHqFyD0hlJMrgvrlceu6goKOISAip3AOw+2ATr66rZ/qkYWRmaLoBEUk8lXsAXlhZS0ubM7N8eNBRRCSkVO4BqIxEGTOkL2NL+wYdRURCSuXeyzbvOsiKLXuYUa4ZIEWk56jce1llJIoZTJ+k6QZEpOeo3HuRu1MViTJl1CCG9c8POo6IhJjKvRdVb93D+7sO6dp2EelxKvde9NTSLeRmZXDtBZpuQER6lsq9l/z8jxuZv2wbn598FkV52UHHEZGQU7n3gkff2MSPfvsON0wo5W9vGBt0HBFJAyr3Hvbkki38r+fW8MlxQ/jJZyeRlak/chHpeWqaHrRg+Tb+pmoVV44p4V++UE62il1Eeonapocsqqnl3gU1XHZuMT+75SJyszStr4j0HpV7D3hx9Xbu+XU1FSMHMudLF5GXrWIXkd6lck+wxWt38I2nVjBheD8e/vLFFORkBR1JRNKQyj2BXnu3ga89voKPDi3i0dsm0ydXxS4iwVC5J8ifNu7izseWcU5JIb/8ymT65etadhEJjso9AZZv3s3seW8xYkABj99+CQMKc4KOJCJpTuUep5qte/jyw28xpCiPJ26/hOI+uUFHEhFRucfj7dq9fOnhpfQvzObJOy5hcFFe0JFERACVe7e9u2M/tz60lMKcTJ68fQql/TSFr4gkD5V7N7zXcIAv/GIJWRnGE3dMYcTAgqAjiYgcQ+V+hrbsOsQXfrEEd+fJOy5hVHFh0JFERE6gC7HPwLYPDvH5X7xJY0srT90xhfMG6wuuRSQ56cy9i7bvbeSLc5ewr7GZx2dfwtjSoqAjiYiclMq9Cxr2H+ELc99k5/4j/PIrkxlf1i/oSCIip6RhmdPYfbCJW+YuoW5PI/O+MpnyswYEHUlE5LS6feZuZiPM7BUzW2tmb5vZXbH1A83sZTNbH7tN2Tbce6iZWx9awvu7DvLQrAomjxoYdCQRkS6JZ1imBfiWu48FpgBfN7NxwH3AYncfDSyOLaec/Y3NfOmRpazfcYCf33oRHzuvOOhIIiJd1u1yd/c6d18Ru78fWAuUAdOBebHd5gEz4szY6w4eaeG2R97i7ehe/u2LF3LFmMFBRxIROSMJeUPVzEYC5cASYIi710H7PwBAp81oZnea2TIzW9bQ0JCIGAnR2NzK7fOWsWLLB/zz58u5etyQoCOJiJyxuMvdzPoAzwB3u/u+rh7n7nPcvcLdK0pKSuKNkRCNza3c+dhy3ty0iwc/M4nrLygNOpKISLfEVe5mlk17sT/h7s/GVu8ws9LY9lKgPr6IvaOppY2/enIFr73bwD/cNIEZ5WVBRxIR6bZ4rpYx4CFgrbs/2GHTImBW7P4sYGH34/WOltY27vpVhN+vrefvZoznMxePCDqSiEhc4rnO/TLgVmCVmVXH1n0P+DEw38xmA1uAm+NK2MNa25xvPV3Db1dv529vGMutU84OOpKISNy6Xe7u/jpgJ9k8tbuP25va2pz7nlnJwupa7r12DLd//JygI4mIJETaTj/g7ty/cDVPL9/GXVNH85dXnBd0JBGRhEnLcnd3fvD8Gp5YsoWvXXEud181OuhIIiIJlXbl7u78w4vreOSN9/nKZaO495oxtL83LCISHmlX7j9dvJ7/+ONGbplyFvdPG6tiF5FQSqty//dXN/CT36/n5ouG84Mbx6vYRSS00qbc5/7nezzw4jqmTxrGj//7BDIyVOwiEl5pUe6P/el9fvjCWq6/YCj/dPNEMlXsIhJyoS/3+W9t5f6Fb3PV2CH89HPlZGWG/n9ZRCTc5V4VifKdZ1fyiY+U8G9fLCdbxS4iaSK0bffCyjq+Ob+aKaMG8fNbLyI3KzPoSCIivSaU5f7ymh3c9asIF509gIe+XEFetopdRNJL6Mr91XX1fP2JFZxf1o+Hv3wxBTn6DnARST+hKvc3Nuzkq48tZ/SQPvzytsn0zcsOOpKISCBCU+5LN+3m9nnLGFVcyOOzL6FfgYpdRNJXKMp9xZYPuO2RpQzrn8djsy9hQGFO0JFERAKV8uW+OrqXWQ8vpbhvLk/eMYWSvrlBRxIRCVxKl/u67fu55aElFOVl8+QdUxhSlBd0JBGRpJDS5T6gIJsLyvrx1B1TKOufH3QcEZGkkdLXCQ4uah9jFxGRY6X0mbuIiHRO5S4iEkIqdxGREFK5i4iEkMpdRCSEVO4iIiGkchcRCSGVu4hICJm7B50BM2sANsfxEMXAzgTF6WmplBVSK6+y9pxUyptKWSG+vGe7e0lnG5Ki3ONlZsvcvSLoHF2RSlkhtfIqa89JpbyplBV6Lq+GZUREQkjlLiISQmEp9zlBBzgDqZQVUiuvsvacVMqbSlmhh/KGYsxdRESOFZYzdxER6UDlLiISQilb7mb2sJnVm9nqoLN0hZmNMLNXzGytmb1tZncFnelkzCzPzJaaWU0s6/8OOtPpmFmmmUXM7Pmgs5yOmb1vZqvMrNrMlgWd53TMrL+ZLTCzd2I/v5cGnakzZjYm9md69L99ZnZ30LlOxszuib2+VpvZU2aW0O8JTdkxdzO7HDgA/NLdxwed53TMrBQodfcVZtYXWA7McPc1AUc7gZkZUOjuB8wsG3gduMvd3ww42kmZ2TeBCqDI3acFnedUzOx9oMLdU+KDNmY2D/hPd59rZjlAgbvvCTjWKZlZJhAFLnH3eD4g2SPMrIz219U4dz9sZvOB37j7o4l6jpQ9c3f314DdQefoKnevc/cVsfv7gbVAWbCpOuftDsQWs2P/Je1ZgJkNB24A5gadJWzMrAi4HHgIwN2bkr3YY6YCG5Ox2DvIAvLNLAsoAGoT+eApW+6pzMxGAuXAkoCjnFRsmKMaqAdedvekzQr8BLgXaAs4R1c58JKZLTezO4MOcxrnAA3AI7Fhr7lmVhh0qC74HPBU0CFOxt2jwD8CW4A6YK+7v5TI51C59zIz6wM8A9zt7vuCznMy7t7q7pOA4cBkM0vKoS8zmwbUu/vyoLOcgcvc/ULgOuDrsSHGZJUFXAj8zN3LgYPAfcFGOrXY0NGNwNNBZzkZMxsATAdGAcOAQjO7JZHPoXLvRbHx62eAJ9z92aDzdEXsV/BXgWuDTXJSlwE3xsaxfwX8NzN7PNhIp+butbHbeqASmBxsolPaBmzr8JvbAtrLPpldB6xw9x1BBzmFq4BN7t7g7s3As8DHEvkEKvdeEnuT8iFgrbs/GHSeUzGzEjPrH7ufT/sP4juBhjoJd/+uuw9395G0/yr+B3dP6BlQIplZYewNdWLDG58EkvaKL3ffDmw1szGxVVOBpLsI4DifJ4mHZGK2AFPMrCDWDVNpfx8uYVK23M3sKeBPwBgz22Zms4POdBqXAbfSfmZ59FKt64MOdRKlwCtmthJ4i/Yx96S/xDBFDAFeN7MaYCnwgru/GHCm0/kG8ETs52ES8H+CjXNyZlYAXE37mXDSiv0mtABYAayivYsTOg1Byl4KKSIiJ5eyZ+4iInJyKncRkRBSuYuIhJDKXUQkhFTuIiIhpHIXEQkhlbuISAj9F2hqDuqkU1fMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "acc_quants = pd.Series(accuracy_list,index=list(range(1,9)))\n",
    "acc_quants.plot()  #不同num_bits下的量化准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，当num_bits=4时，量化基本已经达到最大准确率99%。  \n",
    "这说明使用fp32存储这个小模型实在是浪费，使用int4即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.量化感知训练（QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型训练的问题\n",
    "量化模型我们插入了伪量化节点，经过反量化和量化，能够模拟量化带来的数值误差，实际存储的是反量化的float数值。  \n",
    "但是这样的模型还是不能训练，因为在计算图中，量化操作存在round函数（取整函数），这个函数**梯度几乎处处为0**。  \n",
    "这样我们就会导致反向传播的中梯度为0（链式法则），无法完成训练。  \n",
    "我们可以使用STE尝试解决这个问题。  \n",
    "#### Straight Through Estimator\n",
    "STE方式是，直接跳过伪量化的过程，避开round操作。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。  \n",
    "关键点：对weight伪量化会带来误差，从而降低模型精度。通过反向传播把梯度传给伪量化前的weight，并且**更新伪量化前的weight**， 使其适应量化带来的误差。这就是量化感知训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化感知训练的过程，和普通训练模型没什么差异\n",
    "# 更新的也是量化前weight，伪量化操作只在前向传播中使用引入量化误差，本身并不会修改weight\n",
    "def quantize_aware_training(model, device, train_loader, optimizer, epoch):\n",
    "    lossLayer = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.quantize_forward(data)\n",
    "        loss = lossLayer(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Quantize Aware Training Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize Aware Training Epoch: 1 [3200/60000]\tLoss: 0.019525\n",
      "Quantize Aware Training Epoch: 1 [6400/60000]\tLoss: 0.004293\n",
      "Quantize Aware Training Epoch: 1 [9600/60000]\tLoss: 0.011640\n",
      "Quantize Aware Training Epoch: 1 [12800/60000]\tLoss: 0.007704\n",
      "Quantize Aware Training Epoch: 1 [16000/60000]\tLoss: 0.006910\n",
      "Quantize Aware Training Epoch: 1 [19200/60000]\tLoss: 0.006318\n",
      "Quantize Aware Training Epoch: 1 [22400/60000]\tLoss: 0.001679\n",
      "Quantize Aware Training Epoch: 1 [25600/60000]\tLoss: 0.007990\n",
      "Quantize Aware Training Epoch: 1 [28800/60000]\tLoss: 0.003682\n",
      "Quantize Aware Training Epoch: 1 [32000/60000]\tLoss: 0.003175\n",
      "Quantize Aware Training Epoch: 1 [35200/60000]\tLoss: 0.023034\n",
      "Quantize Aware Training Epoch: 1 [38400/60000]\tLoss: 0.041557\n",
      "Quantize Aware Training Epoch: 1 [41600/60000]\tLoss: 0.057725\n",
      "Quantize Aware Training Epoch: 1 [44800/60000]\tLoss: 0.006525\n",
      "Quantize Aware Training Epoch: 1 [48000/60000]\tLoss: 0.014093\n",
      "Quantize Aware Training Epoch: 1 [51200/60000]\tLoss: 0.008154\n",
      "Quantize Aware Training Epoch: 1 [54400/60000]\tLoss: 0.005280\n",
      "Quantize Aware Training Epoch: 1 [57600/60000]\tLoss: 0.005188\n",
      "Quantize Aware Training Epoch: 2 [3200/60000]\tLoss: 0.008833\n",
      "Quantize Aware Training Epoch: 2 [6400/60000]\tLoss: 0.004807\n",
      "Quantize Aware Training Epoch: 2 [9600/60000]\tLoss: 0.002482\n",
      "Quantize Aware Training Epoch: 2 [12800/60000]\tLoss: 0.002300\n",
      "Quantize Aware Training Epoch: 2 [16000/60000]\tLoss: 0.002299\n",
      "Quantize Aware Training Epoch: 2 [19200/60000]\tLoss: 0.002009\n",
      "Quantize Aware Training Epoch: 2 [22400/60000]\tLoss: 0.010432\n",
      "Quantize Aware Training Epoch: 2 [25600/60000]\tLoss: 0.007203\n",
      "Quantize Aware Training Epoch: 2 [28800/60000]\tLoss: 0.001759\n",
      "Quantize Aware Training Epoch: 2 [32000/60000]\tLoss: 0.014559\n",
      "Quantize Aware Training Epoch: 2 [35200/60000]\tLoss: 0.002717\n",
      "Quantize Aware Training Epoch: 2 [38400/60000]\tLoss: 0.006615\n",
      "Quantize Aware Training Epoch: 2 [41600/60000]\tLoss: 0.002674\n",
      "Quantize Aware Training Epoch: 2 [44800/60000]\tLoss: 0.022772\n",
      "Quantize Aware Training Epoch: 2 [48000/60000]\tLoss: 0.013807\n",
      "Quantize Aware Training Epoch: 2 [51200/60000]\tLoss: 0.007171\n",
      "Quantize Aware Training Epoch: 2 [54400/60000]\tLoss: 0.004003\n",
      "Quantize Aware Training Epoch: 2 [57600/60000]\tLoss: 0.004813\n",
      "Quantize Aware Training Epoch: 3 [3200/60000]\tLoss: 0.004471\n",
      "Quantize Aware Training Epoch: 3 [6400/60000]\tLoss: 0.008793\n",
      "Quantize Aware Training Epoch: 3 [9600/60000]\tLoss: 0.038299\n",
      "Quantize Aware Training Epoch: 3 [12800/60000]\tLoss: 0.013152\n",
      "Quantize Aware Training Epoch: 3 [16000/60000]\tLoss: 0.004458\n",
      "Quantize Aware Training Epoch: 3 [19200/60000]\tLoss: 0.010253\n",
      "Quantize Aware Training Epoch: 3 [22400/60000]\tLoss: 0.023321\n",
      "Quantize Aware Training Epoch: 3 [25600/60000]\tLoss: 0.003033\n",
      "Quantize Aware Training Epoch: 3 [28800/60000]\tLoss: 0.013904\n",
      "Quantize Aware Training Epoch: 3 [32000/60000]\tLoss: 0.015491\n",
      "Quantize Aware Training Epoch: 3 [35200/60000]\tLoss: 0.002920\n",
      "Quantize Aware Training Epoch: 3 [38400/60000]\tLoss: 0.011500\n",
      "Quantize Aware Training Epoch: 3 [41600/60000]\tLoss: 0.004883\n",
      "Quantize Aware Training Epoch: 3 [44800/60000]\tLoss: 0.015901\n",
      "Quantize Aware Training Epoch: 3 [48000/60000]\tLoss: 0.014429\n",
      "Quantize Aware Training Epoch: 3 [51200/60000]\tLoss: 0.005134\n",
      "Quantize Aware Training Epoch: 3 [54400/60000]\tLoss: 0.007372\n",
      "Quantize Aware Training Epoch: 3 [57600/60000]\tLoss: 0.043598\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.99"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行量化感知训练\n",
    "from torch import optim\n",
    "batch_size = 64\n",
    "seed = 1\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False\n",
    ")\n",
    "\n",
    "model_qat = copy.deepcopy(fp32_model)  #从全精度模型获得量化模型\n",
    "model_qat.quantize(num_bits=3)\n",
    "model_qat.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    quantize_aware_training(model_qat,device,train_loader,optimizer,epoch+1)\n",
    "model.eval()\n",
    "model.train() #冻结量化参数\n",
    "quantize_inference(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dfq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bb67ba0ae6effbc54f1e26c9156de107350f6c04280f0ca82fd14e38b65dbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
