{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日\n",
    "资料来源：\n",
    "- 知乎专栏：https://www.zhihu.com/column/c_1258047709686231040\n",
    "- github仓库： https://github.com/Jermmy/pytorch-quantization-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = (max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))   #量化误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam(nn.Module):\n",
    "    # 就是将上面的代码进行了封装\n",
    "    # 继承Module是为了让量化（参数）成为网络的一部分\n",
    "    def __init__(self,num_bits=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bits = num_bits\n",
    "        scale = torch.tensor([], requires_grad=False)\n",
    "        zero_point = torch.tensor([], requires_grad=False)\n",
    "        min = torch.tensor([], requires_grad=False)\n",
    "        max = torch.tensor([], requires_grad=False)\n",
    "        \n",
    "        # 使用register_buffer保存量化参数有以下优点：\n",
    "        # 不产生梯度，不会被注册到parameters中，但也会保存到state_dict中\n",
    "        # 这样就能把模型权重和量化参数都独立地保存到模型里。\n",
    "        self.register_buffer('scale', scale)  \n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        self.register_buffer('min', min)\n",
    "        self.register_buffer('max', max)\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max.nelement() == 0 or self.max <tensor.max():\n",
    "            self.max.data = tensor.max().data   #使用data赋值，避免self.max对象本身\n",
    "        self.max.clamp_(min=0)  #保证self.max大于等于0\n",
    "\n",
    "        if self.min.nelement() == 0  or self.min >tensor.min():\n",
    "            self.min.data = tensor.min().data\n",
    "        self.min.clamp_(max=0)  #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(self.min, self.max, self.num_bits)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def dequantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n",
    "\n",
    "    # 从状态字典加载量化参数\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
    "        key_names = ['scale', 'zero_point', 'min', 'max']\n",
    "        for key in key_names:\n",
    "            value = getattr(self, key)\n",
    "            value.data = state_dict[prefix + key].data\n",
    "            state_dict.pop(prefix + key)\n",
    "\n",
    "    def __str__(self):\n",
    "        info = 'scale:%.10f '  % self.scale\n",
    "        info += 'zero_point:%d '  % self.zero_point\n",
    "        info += 'min:%.6f '  % self.min\n",
    "        info += 'max:%.6f'  % self.max\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([  0., 229., 255.,  77., 178.])\n",
      "q_parm: scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n",
      "q_parm state dict: OrderedDict([('scale', tensor(0.1310)), ('zero_point', tensor(76.)), ('min', tensor(-10.)), ('max', tensor(23.4000))])\n",
      "q_parm_new scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "q_parm.update(x)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_parm:\",q_parm)  #打印量化参数\n",
    "print(\"q_parm state dict:\",q_parm.state_dict()) #打印状态字典\n",
    "torch.save(q_parm.state_dict(),\"./q_parm.pt\")# 保存状态字典\n",
    "\n",
    "q_parm_new = QParam(num_bits=8)  #创建新的量化参数对象\n",
    "q_parm_new.load_state_dict(torch.load(\"./q_parm.pt\")) #从保存的状态字典中加载量化参数\n",
    "print(\"q_parm_new\",q_parm_new)  #打印量化参数，看是否一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import math\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "\n",
    "    def __init__(self,qi=True,qo=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        qi:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        qo: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n",
    "\n",
    "class FakeQuantize(Function):\n",
    "    # 伪量化节点，进行量化和反量化\n",
    "    # 模拟量化前后的误差，这样的float推理和量化后的int推理具有相同的精度\n",
    "\n",
    "    # 反向传播求梯度使用STE，这部分在PTQ不进行反向传播，暂时可以忽略。  \n",
    "    # Function类似于没有参数的Module，继承需要重写前向传播forward和反向传播backward\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        \"\"\"\n",
    "        def forward(ctx,input,*args)\n",
    "            ctx: 不需要手动传入，能够执行一些操作方便求梯度，例如：\n",
    "            ctx.save_for_backward(tensor)  在前向传播时保存一些张量\n",
    "            tensor = ctx.saved_tensors    在反向传播时（backward函数内）也可访问到\n",
    "\n",
    "            input:函数的输入\n",
    "            *args:其他可选参数\n",
    "        \"\"\"\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output:后一层传来的梯度\n",
    "\n",
    "        在QAT涉及到反向传播\n",
    "        对于本层（伪量化节点）,不计算梯度，直接把后一层的梯度往前传\n",
    "\n",
    "        backward的返回值需要和forward对应，代表对应输入的梯度。\n",
    "        由于q_parm为量化参数，不需要计算梯度，因此返回None\n",
    "        因此直接 return grad_output，None\n",
    "        \"\"\"\n",
    "        return grad_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(qi=qi,qo=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "        self.num_bits = num_bits\n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其冻结\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M = self.qw.scale*self.qi.scale / self.qo.scale\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  为什么减去zero_point? 其实就是对应公式里 q_w-Z_w\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "        # 推理过程中顺便统计计算输入x，输出a，权重w的量化参数\n",
    "        # 后量化通过数据校准实现这些。\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qi)  #对q_x进行伪量化\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 对卷积权重q_w进行伪量化，然后和x计算卷积操作\n",
    "        # 注意卷积模块的权重始终是存储原始的weight，只有在前向传播时进行伪量化。\n",
    "        # 反向传播时也是更新的量化前的weight\n",
    "        x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, \n",
    "                     stride=self.conv_module.stride,\n",
    "                     padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                     groups=self.conv_module.groups)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qo)  #对输出a做伪量化\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point  #q_x - Z_x\n",
    "        x = self.conv_module(x)    #(q_w-Z_w)*(q_x-Z_x)\n",
    "        x = self.M * x             #M*sum((q_w-Z_w)*(q_x-Z_x))\n",
    "        x.round_()                 #提前做一步round，加快速度。后面只剩下一个整数加法。\n",
    "        x = x + self.qo.zero_point   #M*sum((q_w-Z_w)*(q_x-Z_x))+Z_a\n",
    "        x.round_()\n",
    "        return x\n",
    "\n",
    "# 相同原理 实现其他网络模块\n",
    "class QLinear(QModule):\n",
    "    #量化线性层\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight.data = self.qw.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.qi.scale * self.qw.scale,\n",
    "                                                   zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight.data)\n",
    "\n",
    "        x = F.linear(x, FakeQuantize.apply(self.fc_module.weight, self.qw), self.fc_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()\n",
    "        return x\n",
    "\n",
    "\n",
    "class QReLU(QModule):\n",
    "    # 构建量化版的Relu函数\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        \n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n",
    "\n",
    "class QMaxPooling2d(QModule):\n",
    "    # 构建量化版的MaxPooling2d函数\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv-BN-Relu融合层的实现\n",
    "一个标准的Conv+BN+relu可以合并到一起，加快推理速度。\n",
    "- **BN折叠到卷积层**  \n",
    "卷积层的输出如下所示:  $$y=\\sum_{i}^{N} w_{i} x_{i}+b$$\n",
    "BN层的输出如下所示:  \n",
    "$$y_{bn} =\\gamma \\frac{y-\\mu_{y}}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}+\\beta$$\n",
    "将卷积层的输出$y$带入到BN层中得到：  \n",
    "$$y_{b n}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}\\left(\\sum_{i}^{N} w_{i} x_{i}+b-\\mu_{y}\\right)+\\beta$$\n",
    "仔细观察，当训练结束后，BN层统计量$\\mu_{y}$ ,$\\sigma_{y}$以及参数$\\gamma$, $\\beta$都已经固定下来。我们可以令$\\gamma^{\\prime}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}$,那么能够得到：\n",
    "$$y_{b n}=\\sum_{i}^{N} \\gamma^{\\prime} w_{i} x_{i}+\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$$\n",
    "上式已经和卷积计算公式非常像了，为了看得更清楚，我们令$w_{i}^{\\prime} = \\gamma^{\\prime}w_{i}$,  $b^{\\prime}=\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$,得到最终的BN层输出\n",
    " $$y_{bn}=\\sum_{i}^{N} w_{i} x_{i}^{\\prime}+b^{\\prime}$$\n",
    "这就和卷积的操作一模一样，因此我们可以把BN层折叠合并到卷积层中。先进行数值变换，在进行矩阵运算。\n",
    "\n",
    "- **Relu折叠到卷积层**  \n",
    "1. 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。  \n",
    "2. 要想保证浮点型Relu和量化型Relu数值的一致性，需要对输入输出使用相同的量化参数,否则无法反量化回正确的实数域。  \n",
    "3. relu会对数值进行截断，实际上在量化过程中也存在截断，就是把scale和偏移zero point后的值截断到（qmin,qmax）。  \n",
    "4. 对于单独的一个Relu层，我们可以直接使用输入的qi在量化数值空间做截断，$relu(q_{x}) = max(q_{x},Z_{x})$。\n",
    "5. 对于合并到conv中的relu层，我们可以使用**输出的qo作为量化参数，对relu的输入x进行量化**，量化的截断过程可以直接实现relu的操作。这是因为在float域，relu的值域是[0,r_max]，对应的量化值域就是[q_min,q_max]。relu函数在float域中，对小于0的数置为0；在量化int域中，就是对小于q_min的数置为q_min。而使用qo对x进行量化，就能保证这一点。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    # 构建量化BN层（与Relu融合）\n",
    "    def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.qb = QParam(num_bits=32)\n",
    "\n",
    "    def fold_bn(self, mean, std):\n",
    "        # 将BN层折叠到Conv层\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std  #r' = r/ sqrt(sigma^2+e)  实际上除以的标准差，加上e是为了避免分母为零\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) #w' = r'*w\n",
    "            if self.conv_module.bias is not None:         # b' = r' * b - r' * mu_y + beta\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias    #返回新的w和b\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        # 训练过程中，更新BN层参数\n",
    "        if self.training:\n",
    "            y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, \n",
    "                            stride=self.conv_module.stride,\n",
    "                            padding=self.conv_module.padding,\n",
    "                            dilation=self.conv_module.dilation,\n",
    "                            groups=self.conv_module.groups)\n",
    "            y = y.permute(1, 0, 2, 3) # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -> C,NHW\n",
    "            # mean = y.mean(1)\n",
    "            # var = y.var(1)\n",
    "            mean = y.mean(1).detach()\n",
    "            var = y.var(1).detach()\n",
    "            self.bn_module.running_mean = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                (1 - self.bn_module.momentum) * var\n",
    "        else:\n",
    "            mean = Variable(self.bn_module.running_mean)\n",
    "            var = Variable(self.bn_module.running_var)\n",
    "\n",
    "        std = torch.sqrt(var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(mean, std)  #折叠后的w和b\n",
    "\n",
    "        self.qw.update(weight.data)\n",
    "\n",
    "        #这里的卷积运算实际上包含了BN层\n",
    "        x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, \n",
    "                stride=self.conv_module.stride,\n",
    "                padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                groups=self.conv_module.groups)\n",
    "\n",
    "        x = F.relu(x)    #后面接上relu函数\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        std = torch.sqrt(self.bn_module.running_var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(self.bn_module.running_mean, std)\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(weight.data)\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias.data = quantize_tensor(bias, scale=self.qi.scale * self.qw.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        \n",
    "        #推理时将Relu合并到量化过程\n",
    "        #这一行包含了relu函数的作用，将x截断到(qmin,qmax)非对称量化         \n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型的构建\n",
    "我们在上面已经得到了基本的网络模块（卷积、池化、线性层等）的量化版本，  \n",
    "现在我们尝试构建一个完整的带有BN层的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    # 定义简单的卷积神经网络，包含两层卷积、2层BN层，1层全连接层\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    # 普通的全精度前向传播，穿插了2次relu函数和最大池化操作\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 我们需要把卷积层、池化层、全连接层转为其量化版本\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    # 量化版本的前向传播，就是各量化网络模块forward的串行堆叠\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    # 对各种量化参数进行固定\n",
    "    # 除了第一层卷积需要qi，其他都是对上一层的qo复用为当前层的qi，因为上一层的输出就是这一层的输入。\n",
    "    # 这里可以优化，伪量化过程中，这一层输出与下一层输入是一个tensor，但是执行了两次统计。\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.qo)\n",
    "        self.qconv2.freeze(qi=self.qconv1.qo)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.qo)\n",
    "        self.qfc.freeze(qi=self.qconv2.qo)\n",
    "\n",
    "    # 实际推理中用到的函数，是对各量化模块的quantize_inference的堆叠，纯整数存储和运算\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.qi.quantize_tensor(x) #对最开始的float输入进行量化\n",
    "\n",
    "        \"\"\"\n",
    "        中间推理都是在int域进行， 配合量化参数进行前向传播。\n",
    "        伪量化是在float域进行，虽有本质不同，但最终与int域反量化结果一致。\n",
    "        \"\"\"\n",
    "        qx = self.qconv1.quantize_inference(qx)    \n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.qo.dequantize_tensor(qx)   #最后一步要反量化，得到实数域的结果\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全精度模型训练\n",
    "这里直接使用仓库提供的 *train.py* 得到在mnist数据集上准确率为99%的FP32模型，权重保存在 *./ckpt/mnist_cnnbn.pt*  \n",
    "```shell\n",
    "python ./train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后训练量化（PTQ）\n",
    "这里会将一个全精度模型转为一个量化模型，并且测试其精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的全精度模型\n",
    "fp32_model = NetBN()\n",
    "fp32_model.load_state_dict(torch.load(\"./ckpt/mnist_cnnbn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是几个用于测试精度的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一定的图片数据进行校准，确定input和中间activation的量化参数\n",
    "# 实际上模型weight的校准并不需要数据，这里为了方便都放在quantize_forward中进行校准\n",
    "from torchvision import datasets,transforms\n",
    "test_batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def direct_quantize(model, test_loader):\n",
    "    # 直接量化，使用一些数据集来校准量化参数,使用200个batch的数据\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_forward(data)\n",
    "        if i % 200 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    # 全精度模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quantize_inference(model, test_loader):\n",
    "    # 量化模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_inference(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp32_model.eval()\n",
    "full_inference(fp32_model,test_loader)  #测试全精度的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n",
      "num_bits 1\n",
      "\n",
      "Test set: Quant Model Accuracy: 10%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 2\n",
      "\n",
      "Test set: Quant Model Accuracy: 30%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 3\n",
      "\n",
      "Test set: Quant Model Accuracy: 84%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 4\n",
      "\n",
      "Test set: Quant Model Accuracy: 98%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 5\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 6\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 7\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 8\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accuracy_list = []\n",
    "for num_bits in range(1,9):  #测试num_bits=[1-8]的准确率\n",
    "    model = copy.deepcopy(fp32_model)\n",
    "    model.quantize(num_bits=num_bits) # 将模型的各个网络模块转为其量化版本\n",
    "    model.eval()\n",
    "    direct_quantize(model,test_loader)  #进行量化校准\n",
    "    model.freeze()                       # 将量化参数固定\n",
    "    print(\"num_bits\",num_bits)\n",
    "    acc = quantize_inference(model, test_loader)  #测试量化准确率\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "acc_quants = pd.Series(accuracy_list,index=list(range(1,9)))\n",
    "acc_quants.plot()  #不同num_bits下的量化准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，当num_bits=4时，量化基本已经达到最大准确率99%。  \n",
    "这说明使用fp32存储这个小模型实在是浪费，使用int4即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.量化感知训练（QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型训练的问题\n",
    "量化模型我们插入了伪量化节点，经过反量化和量化，能够模拟量化带来的数值误差，实际存储的是反量化的float数值。  \n",
    "但是这样的模型还是不能训练，因为在计算图中，量化操作存在round函数（取整函数），这个函数**梯度几乎处处为0**。  \n",
    "这样我们就会导致反向传播的中梯度为0（链式法则），无法完成训练。  \n",
    "我们可以使用STE尝试解决这个问题。  \n",
    "#### Straight Through Estimator\n",
    "STE方式是，直接跳过伪量化的过程，避开round操作。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。  \n",
    "关键点：对weight伪量化会带来误差，从而降低模型精度。通过反向传播把梯度传给伪量化前的weight，并且**更新伪量化前的weight**， 使其适应量化带来的误差。这就是量化感知训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化感知训练的过程，和普通训练模型没什么差异\n",
    "# 更新的也是量化前weight，伪量化操作只在前向传播中使用引入量化误差，本身并不会修改weight\n",
    "def quantize_aware_training(model, device, train_loader, optimizer, epoch):\n",
    "    lossLayer = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.quantize_forward(data)\n",
    "        loss = lossLayer(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Quantize Aware Training Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize Aware Training Epoch: 1 [3200/60000]\tLoss: 0.378004\n",
      "Quantize Aware Training Epoch: 1 [6400/60000]\tLoss: 0.275093\n",
      "Quantize Aware Training Epoch: 1 [9600/60000]\tLoss: 0.452319\n",
      "Quantize Aware Training Epoch: 1 [12800/60000]\tLoss: 0.252710\n",
      "Quantize Aware Training Epoch: 1 [16000/60000]\tLoss: 0.247085\n",
      "Quantize Aware Training Epoch: 1 [19200/60000]\tLoss: 0.270692\n",
      "Quantize Aware Training Epoch: 1 [22400/60000]\tLoss: 0.606104\n",
      "Quantize Aware Training Epoch: 1 [25600/60000]\tLoss: 0.255602\n",
      "Quantize Aware Training Epoch: 1 [28800/60000]\tLoss: 0.396091\n",
      "Quantize Aware Training Epoch: 1 [32000/60000]\tLoss: 0.635318\n",
      "Quantize Aware Training Epoch: 1 [35200/60000]\tLoss: 0.398860\n",
      "Quantize Aware Training Epoch: 1 [38400/60000]\tLoss: 0.422944\n",
      "Quantize Aware Training Epoch: 1 [41600/60000]\tLoss: 0.282550\n",
      "Quantize Aware Training Epoch: 1 [44800/60000]\tLoss: 0.285470\n",
      "Quantize Aware Training Epoch: 1 [48000/60000]\tLoss: 0.291429\n",
      "Quantize Aware Training Epoch: 1 [51200/60000]\tLoss: 0.372722\n",
      "Quantize Aware Training Epoch: 1 [54400/60000]\tLoss: 0.468007\n",
      "Quantize Aware Training Epoch: 1 [57600/60000]\tLoss: 0.193089\n",
      "Quantize Aware Training Epoch: 2 [3200/60000]\tLoss: 0.236014\n",
      "Quantize Aware Training Epoch: 2 [6400/60000]\tLoss: 0.308480\n",
      "Quantize Aware Training Epoch: 2 [9600/60000]\tLoss: 0.390284\n",
      "Quantize Aware Training Epoch: 2 [12800/60000]\tLoss: 0.454882\n",
      "Quantize Aware Training Epoch: 2 [16000/60000]\tLoss: 0.424538\n",
      "Quantize Aware Training Epoch: 2 [19200/60000]\tLoss: 0.233277\n",
      "Quantize Aware Training Epoch: 2 [22400/60000]\tLoss: 0.207083\n",
      "Quantize Aware Training Epoch: 2 [25600/60000]\tLoss: 0.225031\n",
      "Quantize Aware Training Epoch: 2 [28800/60000]\tLoss: 0.229653\n",
      "Quantize Aware Training Epoch: 2 [32000/60000]\tLoss: 0.386215\n",
      "Quantize Aware Training Epoch: 2 [35200/60000]\tLoss: 0.494832\n",
      "Quantize Aware Training Epoch: 2 [38400/60000]\tLoss: 0.332895\n",
      "Quantize Aware Training Epoch: 2 [41600/60000]\tLoss: 0.264604\n",
      "Quantize Aware Training Epoch: 2 [44800/60000]\tLoss: 0.274672\n",
      "Quantize Aware Training Epoch: 2 [48000/60000]\tLoss: 0.649865\n",
      "Quantize Aware Training Epoch: 2 [51200/60000]\tLoss: 0.430395\n",
      "Quantize Aware Training Epoch: 2 [54400/60000]\tLoss: 0.580850\n",
      "Quantize Aware Training Epoch: 2 [57600/60000]\tLoss: 0.672273\n",
      "Quantize Aware Training Epoch: 3 [3200/60000]\tLoss: 0.190316\n",
      "Quantize Aware Training Epoch: 3 [6400/60000]\tLoss: 0.322280\n",
      "Quantize Aware Training Epoch: 3 [9600/60000]\tLoss: 0.339369\n",
      "Quantize Aware Training Epoch: 3 [12800/60000]\tLoss: 0.321924\n",
      "Quantize Aware Training Epoch: 3 [16000/60000]\tLoss: 0.293896\n",
      "Quantize Aware Training Epoch: 3 [19200/60000]\tLoss: 0.396559\n",
      "Quantize Aware Training Epoch: 3 [22400/60000]\tLoss: 0.390623\n",
      "Quantize Aware Training Epoch: 3 [25600/60000]\tLoss: 0.349930\n",
      "Quantize Aware Training Epoch: 3 [28800/60000]\tLoss: 0.364364\n",
      "Quantize Aware Training Epoch: 3 [32000/60000]\tLoss: 0.457664\n",
      "Quantize Aware Training Epoch: 3 [35200/60000]\tLoss: 0.517572\n",
      "Quantize Aware Training Epoch: 3 [38400/60000]\tLoss: 0.415186\n",
      "Quantize Aware Training Epoch: 3 [41600/60000]\tLoss: 0.213325\n",
      "Quantize Aware Training Epoch: 3 [44800/60000]\tLoss: 0.392773\n",
      "Quantize Aware Training Epoch: 3 [48000/60000]\tLoss: 0.478156\n",
      "Quantize Aware Training Epoch: 3 [51200/60000]\tLoss: 0.772059\n",
      "Quantize Aware Training Epoch: 3 [54400/60000]\tLoss: 0.314255\n",
      "Quantize Aware Training Epoch: 3 [57600/60000]\tLoss: 0.322340\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行量化感知训练\n",
    "from torch import optim\n",
    "batch_size = 64\n",
    "seed = 1\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False\n",
    ")\n",
    "\n",
    "model_qat = copy.deepcopy(fp32_model)  #从全精度模型获得量化模型\n",
    "model_qat.quantize(num_bits=3)\n",
    "model_qat.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    quantize_aware_training(model_qat,device,train_loader,optimizer,epoch+1)\n",
    "model.eval()\n",
    "model.train() #冻结量化参数\n",
    "quantize_inference(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在num_bits=3时，QAT只有84%的准确率，而QAT能够达到99%的准确率。  \n",
    "QAT在提升低比特量化更具备优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dfq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bb67ba0ae6effbc54f1e26c9156de107350f6c04280f0ca82fd14e38b65dbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
